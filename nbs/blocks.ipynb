{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e86dc1f",
   "metadata": {},
   "source": [
    "# Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc529b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4724116e0a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# default_exp blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "# default_exp blocks\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from attention_unet.utils import all_equal, test_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b402cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ConvLayer(nn.Sequential):\n",
    "    \" Construct a Sequence of Conv -> BN -> Act \"\n",
    "    @delegates(nn.Conv3d)\n",
    "    def __init__(self,\n",
    "                 in_c, # number of input channels\n",
    "                 out_c, # number of output channels\n",
    "                 ks=3, # kernel size (tuple or int)\n",
    "                 stride=1, # kernel stride (tuple or int)\n",
    "                 padding='auto', # padding during convolution, if auto padding is calcualted automatically\n",
    "                 pad_value=0., # value to pad input with\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 transpose=False, # if transpose convolution should be constructed\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        layers = OrderedDict([])\n",
    "        \n",
    "        # asymmetric padding\n",
    "        if padding=='auto': \n",
    "            if isinstance(ks, int): ks = (ks, )*3\n",
    "            padding = [pad for _ks in ks for pad in self.calculate_padding(_ks)]\n",
    "            if all_equal(padding): padding = padding[0]\n",
    "            else: layers['pad'] = nn.ConstantPad3d(padding[::-1], value=pad_value)\n",
    "        \n",
    "        # Conv Layer    \n",
    "        Conv = nn.ConvTranspose3d if transpose else nn.Conv3d\n",
    "        conv_layer = Conv(in_c, out_c, ks, stride=stride, \n",
    "                          padding=0 if len(layers) == 1 else padding, **kwargs)\n",
    "        \n",
    "        layers['transpose_conv' if transpose else 'conv'] =  conv_layer\n",
    "\n",
    "        if norm: layers['norm'] = norm(out_c)\n",
    "        if act: layers['act'] = act()\n",
    "        \n",
    "        # create layers\n",
    "        super().__init__(layers)\n",
    "  \n",
    "    def calculate_padding(self, ks):\n",
    "        if ks % 2 == 0: return ks // 2, (ks-1) //2\n",
    "        else: return ks //2, ks // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(ConvLayer(3,3)) # with stride 1 input size should be equal output size\n",
    "test_forward(ConvLayer(3,3,transpose=True))\n",
    "test_forward(ConvLayer(3,3,stride=2), check_size=False)\n",
    "test_forward(ConvLayer(3,3,transpose=True, stride=2), check_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DropConnect(nn.Module):\n",
    "    \" Drops connections with probability p \"\n",
    "    def __init__(self, \n",
    "                 p # percentage to drop\n",
    "                ):\n",
    "        assert 0 <= p <= 1, 'p must be in range of [0,1]'\n",
    "        self.p = 1 - p # percentage to KEEP\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # generate binary_tensor mask according to probability (p for 0, 1-p for 1)\n",
    "        random_tensor = self.p + torch.rand([batch_size, 1, 1, 1, 1], dtype=x.dtype, \n",
    "                                            device=x.device)\n",
    "        return x / self.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(DropConnect(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SqueezeExpand(nn.Module): \n",
    "    \"Squeeze Excitation Layer\"\n",
    "    @delegates(nn.Conv3d)\n",
    "    def __init__(self,\n",
    "                 in_c, # number of input channels\n",
    "                 se_ratio, # squeeze-expand ratio\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(SqueezeExpand, self).__init__()\n",
    "        num_squeezed_channels = max(1, int(in_c * se_ratio))\n",
    "        self.squeeze_expand = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('pool', nn.AdaptiveAvgPool3d(1)),\n",
    "                ('squeeze', ConvLayer(in_c=in_c, out_c=num_squeezed_channels, ks=1,\n",
    "                            act=act, norm=None, **kwargs)),\n",
    "                ('expand',  ConvLayer(in_c=num_squeezed_channels, out_c=in_c, ks=1,\n",
    "                            act=None, norm=None,**kwargs)),\n",
    "                ('sigmoid', nn.Sigmoid())])\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return x * self.squeeze_expand(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Residual Bottleneck Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_c, # number of input channels\n",
    "                 out_c, # number of output channels\n",
    "                 ks, # size of convolution kernel\n",
    "                 stride, # stride of kernel\n",
    "                 se_ratio, # squeeze-expand ratio\n",
    "                 id_skip, # if skip connection shouldbe used\n",
    "                 expand_ratio, # expansion ratio for inverted bottleneck\n",
    "                 drop_connect_rate = 0.2, # percentage of dropped connections\n",
    "                 act=nn.SiLU, # type of activation function\n",
    "                 norm=nn.BatchNorm3d, # type of batch normalization\n",
    "                 **kwargs # further arguments passed to `ConvLayerDynamicPadding`\n",
    "                ):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        store_attr()\n",
    "\n",
    "        # expansion phase (inverted bottleneck)\n",
    "        n_intermed = in_c * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1:\n",
    "            self.expand_conv = ConvLayer(in_c=in_c, out_c=n_intermed,\n",
    "                                         ks = 1,norm=norm,\n",
    "                                         act=act, **kwargs)\n",
    "\n",
    "        # depthwise convolution phase, groups makes it depthwise\n",
    "        self.depthwise_conv = ConvLayer(in_c=n_intermed, out_c=n_intermed,\n",
    "                                        groups=n_intermed, ks=ks,\n",
    "                                        stride=stride, norm=norm,\n",
    "                                        act=act, **kwargs)\n",
    "\n",
    "        # squeeze and excitation layer, if desired\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        if self.has_se: \n",
    "            self.squeeze_expand = SqueezeExpand(in_c=n_intermed, se_ratio=se_ratio, \n",
    "                                                act=act, norm=norm)\n",
    "\n",
    "        # pointwise convolution phase\n",
    "        self.project_conv = ConvLayer(in_c=n_intermed, out_c=out_c, ks=1,\n",
    "                                      act = None, **kwargs)\n",
    "        \n",
    "        self.drop_conncet = DropConnect(drop_connect_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.id_skip: inputs = x # save input only if skip connection\n",
    "            \n",
    "        # expansion\n",
    "        if self.expand_ratio != 1: x = self.expand_conv(x)\n",
    "        \n",
    "        # depthwise convolution\n",
    "        x = self.depthwise_conv(x)\n",
    "        \n",
    "        # squeeze and excitation (self attention)\n",
    "        if self.has_se:  x = self.squeeze_expand(x) * x\n",
    "        \n",
    "        # pointwise convolution\n",
    "        x = self.project_conv(x)\n",
    "        \n",
    "        # skip connection and drop connect\n",
    "        if self.id_skip and self.stride == 1 and self.in_c == self.out_c:\n",
    "            x = self.drop_conncet(x) + inputs  # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(MBConvBlock(3,3,3,1,0.2,True,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"Apply attention gate to input in U-Net Block. Adapted from arxiv.org/abs/1804.03999\"\n",
    "    def __init__(self, \n",
    "                 in_c, # number of input channels\n",
    "                 ks = 7, \n",
    "                 max_pool=True, \n",
    "                 mean_pool=True, \n",
    "                 xtra_conv=False,\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        store_attr()\n",
    "        \n",
    "        if self.xtra_conv: \n",
    "            self.xtra_conv_layer = ConvLayer(in_c, 1, ks=ks, **kwargs)\n",
    "        \n",
    "        n_final = max_pool + mean_pool + xtra_conv\n",
    "        assert n_final > 0, 'No pooling layers in `SpatialAttention`-block'\n",
    "        self.out_conv = ConvLayer(n_final, 1, ks=ks, act=nn.Sigmoid, norm=None, **kwargs)\n",
    "        \n",
    "           \n",
    "    def forward(self, x):\n",
    "        compressed = list()\n",
    "        if self.max_pool: compressed.append(x.max(1)[0].unsqueeze(1))\n",
    "        if self.mean_pool: compressed.append(x.mean(1).unsqueeze(1))\n",
    "        if self.xtra_conv: compressed.append(self.xtra_conv_layer(x))\n",
    "        return self.out_conv(torch.cat(compressed, 1)) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41444e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(SpatialAttention(3))\n",
    "test_forward(SpatialAttention(16, xtra_conv=True))\n",
    "test_forward(SpatialAttention(8, max_pool=False))\n",
    "test_forward(SpatialAttention(2, mean_pool=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SpatialAttentionDualInput(nn.Module):\n",
    "    \"Apply attention gate to input in U-Net Block. Adapted from arxiv.org/abs/1804.03999\"\n",
    "    def __init__(self, \n",
    "                 in_c, # number of input channels\n",
    "                 s_c, # number of gated channels\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(SpatialAttentionDualInput, self).__init__()\n",
    "        self.conv_u = ConvLayer(in_c, s_c, ks=1, stride=1, act=None, norm=None)\n",
    "        self.conv_s = ConvLayer(s_c, s_c, ks=2, stride=2,  act=None, norm=None, bias = False)\n",
    "        self.conv_attn = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            ConvLayer(s_c, 1, ks=1,  act=nn.Sigmoid, stride=1, **kwargs),\n",
    "        )\n",
    "           \n",
    "    def forward(self, up_in, s):\n",
    "        x = self.conv_u(up_in)\n",
    "        s = F.interpolate(self.conv_s(s), size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        attn_gate = F.interpolate(self.conv_attn(x + s), size=up_in.shape[2:], mode='trilinear', align_corners=False)\n",
    "        return up_in * attn_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44840d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UnetBlock(nn.Module):\n",
    "    \" Create a U-Net Block \"\n",
    "    @delegates(nn.ConvTranspose3d)\n",
    "    def __init__(self, \n",
    "                 in_c, # number of input channels\n",
    "                 s_c, # number of gated channels\n",
    "                 ks=3, # kernel size for upsampling layer\n",
    "                 stride=2, # stride for upsampling layer\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 spatial_attention=False, # use spatial attention on input\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(UnetBlock, self).__init__()\n",
    "        up_c = in_c # in_c is used with mist Modules, but up_c would be a more suitable name for this block\n",
    "        \n",
    "        self.up = ConvLayer(up_c, up_c//2, ks=ks, stride=stride, act=act, norm=None, transpose=True, **kwargs)\n",
    "        self.bn = norm(s_c)\n",
    "        if spatial_attention: self.sa = SpatialAttentionDualInput(up_c, s_c)\n",
    "        \n",
    "        in_c = up_c // 2 + s_c\n",
    "        out_c = in_c // 2\n",
    "        \n",
    "        self.final_conv = nn.Sequential(\n",
    "            act(),\n",
    "            ConvLayer(in_c, out_c, act=act, norm=norm, **kwargs),\n",
    "            ConvLayer(out_c, out_c, act=act, norm=norm, **kwargs) # ks = ?\n",
    "        )\n",
    "\n",
    "    def forward(self, up_in, s):\n",
    "        s = self.bn(s)\n",
    "        if hasattr(self, 'sa'): up_in = self.sa(up_in, s) \n",
    "        up_out = self.up(up_in)\n",
    "        if s.shape[-3:] != up_out.shape[-3:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-3:], mode='nearest')\n",
    "        cat_x = torch.cat([up_out, s], dim=1)\n",
    "        return self.final_conv(cat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad89b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicResBlock(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, out_c, ks=3, stride=1, padding='auto', downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm=nn.BatchNorm3d, act=nn.ReLU, **kwargs):\n",
    "        super(BasicResBlock, self).__init__()\n",
    "\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv = nn.Sequential(OrderedDict([\n",
    "            ('conv_layer_1', ConvLayer(in_c, out_c, ks=ks, stride=stride, padding=padding, norm=norm, act=act, **kwargs)),\n",
    "            ('conv_layer_2', ConvLayer(out_c, out_c, ks=ks, stride=1, padding=padding, norm=norm, act=None, **kwargs))])\n",
    "                                 )\n",
    "        if stride != 1 or in_c != out_c: \n",
    "            self.downsample = ConvLayer(in_c, out_c, ks=1, stride=stride, norm=norm, act=None, **kwargs)\n",
    "        else: self.downsample = nn.Identity()\n",
    "        \n",
    "        self.final_act = act()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) + self.downsample(x)\n",
    "        return self.final_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47477af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3ebebfb25875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBasicResBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_forward' is not defined"
     ]
    }
   ],
   "source": [
    "test_forward(BasicResBlock(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80512fe7-3eb9-4b57-a3a4-da5371647949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BottleneckResBlock(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, out_c, ks=3, stride=1, padding='auto', downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm=nn.BatchNorm3d, act=nn.ReLU, **kwargs):\n",
    "        super(BasicResBlock, self).__init__()\n",
    "\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv = nn.Sequential(OrderedDict([\n",
    "            ('conv_layer_1', ConvLayer(in_c, out_c, ks=ks, stride=stride, padding=padding, norm=norm, act=act, **kwargs)),\n",
    "            ('conv_layer_2', ConvLayer(out_c, out_c, ks=ks, stride=1, padding=padding, norm=norm, act=None, **kwargs))])\n",
    "                                 )\n",
    "        if stride != 1 or in_c != out_c: \n",
    "            self.downsample = ConvLayer(in_c, out_c, ks=1, stride=stride, norm=norm, act=None, **kwargs)\n",
    "        else: self.downsample = nn.Identity()\n",
    "        \n",
    "        self.final_act = act()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) + self.downsample(x)\n",
    "        return self.final_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83137534-aa9a-4292-ae9c-9c39e3fc4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(BasicResBlock(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63188022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DoubleConv(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, **kwargs):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv1 = ConvLayer(in_c, in_c*2)\n",
    "        self.conv2 = ConvLayer(in_c*2, in_c)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.conv2(self.conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(DoubleConv(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DeepSupervision(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, out_c, ks=1, act=None, norm=None, **kwargs):\n",
    "        super(DeepSupervision, self).__init__()\n",
    "        assert out_c > 1, f'Expected `out_c` to be at least 2 but got {out_c}'\n",
    "        self.conv = nn.Sequential(\n",
    "            ConvLayer(in_c, out_c, ks=ks, act=act, norm=norm), \n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.conv(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(DeepSupervision(3,3))\n",
    "test_forward(DeepSupervision(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4930e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# deprecated\n",
    "@delegates(BasicResBlock)\n",
    "def res_blocks(in_c, out_c, stride, n_blocks, **kwargs):\n",
    "    blocks = OrderedDict([('block_0', BasicResBlock(in_c, out_c, stride=stride, **kwargs))])\n",
    "    if n_blocks == 1: return nn.Sequential(blocks)\n",
    "    for i in range(n_blocks - 1):\n",
    "        blocks[f'block_{i+1}'] = BasicResBlock(out_c, out_c, stride=1, **kwargs)\n",
    "    return nn.Sequential(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90beeb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted blocks.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted models.ipynb.\n",
      "Converted modular_unet.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75707e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff2d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
