{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff75b0b-c8ff-46cc-a8d9-1084e792b3ed",
   "metadata": {},
   "source": [
    "# Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92aaffa-4f7c-4ba9-bbf2-63545d31414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# default_exp blocks\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9b6d0-dcba-47c1-987e-2177d8e158e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from attention_unet.utils import all_equal, test_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e27ee-a803-4cb4-841f-d51ead772c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ConvLayer(nn.Sequential):\n",
    "    \" Construct a Sequence of Conv -> BN -> Act \"\n",
    "    @delegates(nn.Conv3d)\n",
    "    def __init__(self,\n",
    "                 in_c, # number of input channels\n",
    "                 out_c, # number of output channels\n",
    "                 ks=3, # kernel size (tuple or int)\n",
    "                 stride=1, # kernel stride (tuple or int)\n",
    "                 padding='auto', # padding during convolution, if auto padding is calcualted automatically\n",
    "                 pad_value=0., # value to pad input with\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 transpose=False, # if transpose convolution should be constructed\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        layers = OrderedDict([])\n",
    "        \n",
    "        # asymmetric padding\n",
    "        if padding=='auto': \n",
    "            if isinstance(ks, int): ks = (ks, )*3\n",
    "            padding = [pad for _ks in ks for pad in self.calculate_padding(_ks)]\n",
    "            if all_equal(padding): padding = padding[0]\n",
    "            else: layers['pad'] = nn.ConstantPad3d(padding[::-1], value=pad_value)\n",
    "        \n",
    "        # Conv Layer    \n",
    "        Conv = nn.ConvTranspose3d if transpose else nn.Conv3d\n",
    "        conv_layer = Conv(in_c, out_c, ks, stride=stride, \n",
    "                          padding=0 if len(layers) == 1 else padding, **kwargs)\n",
    "        \n",
    "        layers['transpose_conv' if transpose else 'conv'] =  conv_layer\n",
    "\n",
    "        if norm: layers['norm'] = norm(out_c)\n",
    "        if act: layers['act'] = act()\n",
    "        \n",
    "        # create layers\n",
    "        super().__init__(layers)\n",
    "  \n",
    "    def calculate_padding(self, ks):\n",
    "        if ks % 2 == 0: return ks // 2, (ks-1) //2\n",
    "        else: return ks //2, ks // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6c00e-e282-416d-87d5-d2b5536da3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(ConvLayer(3,3)) # with stride 1 input size should be equal output size\n",
    "test_forward(ConvLayer(3,3,transpose=True))\n",
    "test_forward(ConvLayer(3,3,stride=2), check_size=False)\n",
    "test_forward(ConvLayer(3,3,transpose=True, stride=2), check_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c80760-2fc0-44f5-8bf6-89560618030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DropConnect(nn.Module):\n",
    "    \" Drops connections with probability p \"\n",
    "    def __init__(self, \n",
    "                 p # percentage to drop\n",
    "                ):\n",
    "        assert 0 <= p <= 1, 'p must be in range of [0,1]'\n",
    "        self.p = 1 - p # percentage to KEEP\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # generate binary_tensor mask according to probability (p for 0, 1-p for 1)\n",
    "        random_tensor = self.p + torch.rand([batch_size, 1, 1, 1, 1], dtype=x.dtype, \n",
    "                                            device=x.device)\n",
    "        return x / self.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a899ef-d550-4ee1-917e-9bb4201d1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(DropConnect(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637f653-033e-4fb7-a8be-61e744966dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SqueezeExpand(nn.Module): \n",
    "    \"Squeeze Excitation Layer\"\n",
    "    @delegates(nn.Conv3d)\n",
    "    def __init__(self,\n",
    "                 in_c, # number of input channels\n",
    "                 se_ratio, # squeeze-expand ratio\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(SqueezeExpand, self).__init__()\n",
    "        num_squeezed_channels = max(1, int(in_c * se_ratio))\n",
    "        self.squeeze_expand = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('pool', nn.AdaptiveAvgPool3d(1)),\n",
    "                ('squeeze', ConvLayer(in_c=in_c, out_c=num_squeezed_channels, ks=1,\n",
    "                            act=act, norm=None, **kwargs)),\n",
    "                ('expand',  ConvLayer(in_c=num_squeezed_channels, out_c=in_c, ks=1,\n",
    "                            act=None, norm=None,**kwargs)),\n",
    "                ('sigmoid', nn.Sigmoid())])\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return x * self.squeeze_expand(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7bfb7-dc7f-4788-b7e6-1410e87fec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Residual Bottleneck Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_c, # number of input channels\n",
    "                 out_c, # number of output channels\n",
    "                 ks, # size of convolution kernel\n",
    "                 stride, # stride of kernel\n",
    "                 se_ratio, # squeeze-expand ratio\n",
    "                 id_skip, # if skip connection shouldbe used\n",
    "                 expand_ratio, # expansion ratio for inverted bottleneck\n",
    "                 drop_connect_rate = 0.2, # percentage of dropped connections\n",
    "                 act=nn.SiLU, # type of activation function\n",
    "                 norm=nn.BatchNorm3d, # type of batch normalization\n",
    "                 **kwargs # further arguments passed to `ConvLayerDynamicPadding`\n",
    "                ):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        store_attr()\n",
    "\n",
    "        # expansion phase (inverted bottleneck)\n",
    "        n_intermed = in_c * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1:\n",
    "            self.expand_conv = ConvLayer(in_c=in_c, out_c=n_intermed,\n",
    "                                         ks = 1,norm=norm,\n",
    "                                         act=act, **kwargs)\n",
    "\n",
    "        # depthwise convolution phase, groups makes it depthwise\n",
    "        self.depthwise_conv = ConvLayer(in_c=n_intermed, out_c=n_intermed,\n",
    "                                        groups=n_intermed, ks=ks,\n",
    "                                        stride=stride, norm=norm,\n",
    "                                        act=act, **kwargs)\n",
    "\n",
    "        # squeeze and excitation layer, if desired\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        if self.has_se: \n",
    "            self.squeeze_expand = SqueezeExpand(in_c=n_intermed, se_ratio=se_ratio, \n",
    "                                                act=act, norm=norm)\n",
    "\n",
    "        # pointwise convolution phase\n",
    "        self.project_conv = ConvLayer(in_c=n_intermed, out_c=out_c, ks=1,\n",
    "                                      act = None, **kwargs)\n",
    "        \n",
    "        self.drop_conncet = DropConnect(drop_connect_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.id_skip: inputs = x # save input only if skip connection\n",
    "            \n",
    "        # expansion\n",
    "        if self.expand_ratio != 1: x = self.expand_conv(x)\n",
    "        \n",
    "        # depthwise convolution\n",
    "        x = self.depthwise_conv(x)\n",
    "        \n",
    "        # squeeze and excitation (self attention)\n",
    "        if self.has_se:  x = self.squeeze_expand(x) * x\n",
    "        \n",
    "        # pointwise convolution\n",
    "        x = self.project_conv(x)\n",
    "        \n",
    "        # skip connection and drop connect\n",
    "        if self.id_skip and self.stride == 1 and self.in_c == self.out_c:\n",
    "            x = self.drop_conncet(x) + inputs  # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aea833-ec68-4642-ba27-d3bcdbab2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(MBConvBlock(3,3,3,1,0.2,True,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e260d-5a2a-4791-8a2a-7ce4794cf140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SpatialAttentionDualInput(nn.Module):\n",
    "    \"Apply attention gate to input in U-Net Block. Adapted from arxiv.org/abs/1804.03999\"\n",
    "    def __init__(self, \n",
    "                 in_c, # number of input channels\n",
    "                 s_c, # number of gated channels\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv_u = ConvLayer(in_c, s_c, ks=1, stride=1, act=None, norm=None)\n",
    "        self.conv_s = ConvLayer(s_c, s_c, ks=2, stride=2,  act=None, norm=None, bias = False)\n",
    "        self.conv_attn = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            ConvLayer(s_c, 1, ks=1,  act=nn.Sigmoid, stride=1, **kwargs),\n",
    "        )\n",
    "           \n",
    "    def forward(self, up_in, s):\n",
    "        x = self.conv_u(up_in)\n",
    "        s = F.interpolate(self.conv_s(s), size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        attn_gate = F.interpolate(self.conv_attn(x + s), size=up_in.shape[2:], mode='trilinear', align_corners=False)\n",
    "        return up_in * attn_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc572393-1c8a-4157-aeb7-f33a4d87ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UnetBlock(nn.Module):\n",
    "    \" Create a U-Net Block \"\n",
    "    @delegates(nn.ConvTranspose3d)\n",
    "    def __init__(self, \n",
    "                 in_c, # number of input channels\n",
    "                 s_c, # number of gated channels\n",
    "                 ks=3, # kernel size for upsampling layer\n",
    "                 stride=2, # stride for upsampling layer\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 spatial_attention=False, # use spatial attention on input\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ):\n",
    "        super(UnetBlock, self).__init__()\n",
    "        up_c = in_c # in_c is used with mist Modules, but up_c would be a more suitable name for this block\n",
    "        \n",
    "        self.up = ConvLayer(up_c, up_c//2, ks=ks, stride=stride, act=act, norm=None, transpose=True, **kwargs)\n",
    "        self.bn = norm(s_c)\n",
    "        if spatial_attention: self.sa = SpatialAttentionDualInput(up_c, s_c)\n",
    "        \n",
    "        in_c = up_c // 2 + s_c\n",
    "        out_c = in_c // 2\n",
    "        \n",
    "        self.final_conv = nn.Sequential(\n",
    "            act(),\n",
    "            ConvLayer(in_c, out_c, act=act, norm=norm, **kwargs),\n",
    "            ConvLayer(out_c, out_c, act=act, norm=norm, **kwargs) # ks = ?\n",
    "        )\n",
    "\n",
    "    def forward(self, up_in, s):\n",
    "        s = self.bn(s)\n",
    "        if hasattr(self, 'sa'): up_in = self.sa(up_in, s) \n",
    "        up_out = self.up(up_in)\n",
    "        if s.shape[-3:] != up_out.shape[-3:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-3:], mode='nearest')\n",
    "        cat_x = torch.cat([up_out, s], dim=1)\n",
    "        return self.final_conv(cat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbccf2f1-45bf-4043-bc36-39cebddaf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicResBlock(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, out_c, ks=3, stride=1, padding='auto', downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm=nn.BatchNorm3d, act=nn.ReLU, **kwargs):\n",
    "        super(BasicResBlock, self).__init__()\n",
    "\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv = nn.Sequential(OrderedDict([\n",
    "            ('conv_layer_1', ConvLayer(in_c, out_c, ks=ks, stride=stride, padding=padding, norm=norm, act=act, **kwargs)),\n",
    "            ('conv_layer_2', ConvLayer(out_c, out_c, ks=ks, stride=1, padding=padding, norm=norm, act=None, **kwargs))])\n",
    "                                 )\n",
    "        if stride != 1 or in_c != out_c: \n",
    "            self.downsample = ConvLayer(in_c, out_c, ks=1, stride=stride, norm=norm, act=None, **kwargs)\n",
    "        else: self.downsample = nn.Identity()\n",
    "        \n",
    "        self.final_act = act()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) + self.downsample(x)\n",
    "        return self.final_act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94eb14-c869-4ab7-bc19-1222ed9c0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(BasicResBlock(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051ce13-e3e0-4f13-af86-84c911a013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DoubleConv(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, **kwargs):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv1 = ConvLayer(in_c, in_c*2)\n",
    "        self.conv2 = ConvLayer(in_c*2, in_c)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.conv2(self.conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531f112-3781-47ff-847f-5a9d4538a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(DoubleConv(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d720d-15ed-49cc-a066-d39e70d9e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DeepSupervision(nn.Module):\n",
    "    @delegates(ConvLayer)\n",
    "    def __init__(self, in_c, out_c, ks=1, act=None, norm=None, **kwargs):\n",
    "        super(DeepSupervision, self).__init__()\n",
    "        assert out_c > 1, f'Expected `out_c` to be at least 2 but got {out_c}'\n",
    "        self.conv = nn.Sequential(\n",
    "            ConvLayer(in_c, out_c, ks=ks, act=act, norm=norm), \n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.conv(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb04f7-c5fe-4ef6-93e2-1125ce0ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forward(DeepSupervision(3,3))\n",
    "test_forward(DeepSupervision(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df922bf-0ad2-499f-85b0-f10421a1826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# deprecated\n",
    "@delegates(BasicResBlock)\n",
    "def res_blocks(in_c, out_c, stride, n_blocks, **kwargs):\n",
    "    blocks = OrderedDict([('block_0', BasicResBlock(in_c, out_c, stride=stride, **kwargs))])\n",
    "    if n_blocks == 1: return nn.Sequential(blocks)\n",
    "    for i in range(n_blocks - 1):\n",
    "        blocks[f'block_{i+1}'] = BasicResBlock(out_c, out_c, stride=1, **kwargs)\n",
    "    return nn.Sequential(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213d050-7e62-4d92-989a-2a8038e8e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted blocks.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted models.ipynb.\n",
      "Converted modular_unet.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7cf9a-adb2-4ed0-8cb3-c594055675c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
