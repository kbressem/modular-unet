{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e34beb-3b5c-405d-b713-2e4550c83b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# default_exp modular_unet\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "from fastcore.dispatch import patch\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014aa5e-420f-46ed-9973-c90e82a92037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from attention_unet.blocks import UnetBlock, ConvLayer, SqueezeExpand, BasicResBlock\n",
    "from attention_unet.utils import hasattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4c018-760f-4e27-af1d-2f311c0f1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ModularUNet(nn.Module): \n",
    "    \" Modular 3D UNet \"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_c:int, # number of input channels\n",
    "                 n_classes:int, # number of output channels / number of classes\n",
    "                 norm=nn.BatchNorm3d, # type of batch nornalization\n",
    "                 act=nn.ReLU, # activation function\n",
    "                 **kwargs # further arguments for ConvLayer\n",
    "                ): \n",
    "        super(ModularUNet, self).__init__()\n",
    "        store_attr(store_args=True) # saves all attributes to self\n",
    "        \n",
    "        hasattrs(self, ('channels', 'kernel_size', 'stride', 'padding', 'n_layers', 'n_blocks'), do_raise=True)\n",
    "        \n",
    "        # encoder layers (downsampling)\n",
    "        original_in_c = in_c\n",
    "        for i in range(self.n_blocks):\n",
    "            setattr(self, \n",
    "                    f'encoder_block_{i}', \n",
    "                    self.create_encoder_block(in_c, self.channels[i], self.kernel_size[i], \n",
    "                                              self.stride[i], self.padding[i], self.n_layers[i], **kwargs)\n",
    "                   )\n",
    "            in_c = self.channels[i]\n",
    "        \n",
    "        # skip blocks (applied to gated inputs from skip connections before passed to extra layer or upsampling)\n",
    "        for i in range(self.n_blocks-1): # the last skip_block is essentially the middle conv\n",
    "            setattr(self, \n",
    "                    f'skip_block_{i}', \n",
    "                    self.create_skip_block(self.channels[i], **kwargs)\n",
    "                   )\n",
    "            \n",
    "        # Middle Conv\n",
    "        self.middle_block = self.create_middle_block(self.channels[-1], **kwargs) \n",
    "            \n",
    "        # Decoder (upsampling)\n",
    "        for i in reversed(range(self.n_blocks)): \n",
    "            s_c = self.channels[i-1] if i > 0 else original_in_c\n",
    "            setattr(self, f'decoder_block_{i}', self.create_decoder_block(in_c=self.channels[i], s_c=s_c, **kwargs))\n",
    "            \n",
    "            in_c = (s_c + self.channels[i]//2) // 2\n",
    "            setattr(self, f'extra_after_decoder_block_{i}', \n",
    "                    self.create_extra_after_decoder_block(in_c=in_c, out_c=n_classes, **kwargs))\n",
    "            \n",
    "        # Final layer (n_channels -> n_classes)\n",
    "        if not isinstance(self.extra_after_decoder_block_0, nn.Identity): \n",
    "            in_c = n_classes * self.n_blocks\n",
    "        self.final_block = self.create_final_block(in_c, n_classes)\n",
    "     \n",
    "    # Each block is created with a `create`_block` function, which passes arguments to the `layer` function\n",
    "    # The `layer` function can either be patched to ModularUnet or added in a subclass.\n",
    "    def create_encoder_block(self, in_c, out_c, ks, stride, padding, n_layers, **kwargs): \n",
    "        \" Create `n_layers` instances of `encoder_layer` \"        \n",
    "        assert hasattr(self, 'encoder_layer'), self._not_implemented_error('encoder_layer')\n",
    "        layers = OrderedDict([('layer_0', self.encoder_layer(in_c=in_c, out_c=out_c, ks=ks, stride=stride, \n",
    "                                                            padding=padding, **kwargs))])\n",
    "        if n_layers == 1: return nn.Sequential(layers)\n",
    "        for i in range(n_layers - 1):\n",
    "            layers[f'layer_{i+1}'] = self.encoder_layer(in_c=out_c, out_c=out_c, ks=ks, stride=stride, \n",
    "                                                        padding=padding, **kwargs)\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def create_skip_block(self, in_c, **kwargs):\n",
    "        \" Build skip blocks \"\n",
    "        assert hasattr(self, 'skip_layer'), self._not_implemented_error('skip_layer')\n",
    "        return self.skip_layer(in_c=in_c, **kwargs)\n",
    "    \n",
    "    def create_middle_block(self, in_c, **kwargs): \n",
    "        \" Build middle layer \"\n",
    "        assert hasattr(self, 'middle_layer'), self._not_implemented_error('middle_layer')\n",
    "        return  self.middle_layer(in_c=in_c, **kwargs)\n",
    "    \n",
    "    def create_decoder_block(self, in_c, s_c, **kwargs): \n",
    "        \" Build decoder layers \"\n",
    "        assert hasattr(self, 'decoder_layer'), self._not_implemented_error('decoder_layer')\n",
    "        return self.decoder_layer(in_c=in_c, s_c=s_c, **kwargs)\n",
    "    \n",
    "    def create_extra_after_decoder_block(self, in_c, out_c, **kwargs): \n",
    "        \" Build extra layers applied to each output of the decoder blocks\"\n",
    "        assert hasattr(self, 'extra_after_decoder_layer'), self._not_implemented_error('extra_after_decoder_layer')\n",
    "        return self.extra_after_decoder_layer(in_c=in_c, out_c=out_c, **kwargs)\n",
    "    \n",
    "    def create_final_block(self, in_c, out_c, **kwargs):\n",
    "        \" Build last layer. Defaults to simple ConvLayer if no further arguments are given. \"\n",
    "        if hasattr(self, 'final_layer'): \n",
    "            return self.final_layer(in_c=in_c, out_c=out_c)\n",
    "        return ConvLayer(in_c, out_c, ks=1, **kwargs, act=None, norm=None)            \n",
    "    \n",
    "    # implement forward functions for each part of the Modular UNet\n",
    "    def forward_encoder(self, x): \n",
    "        \" x -> [x1, x2, .. , xn]. Store x1, .. , xn, return xn\"\n",
    "        # downsampling\n",
    "        self.s = [x] # collect outouts for skip connections\n",
    "        for i in range(self.n_blocks): \n",
    "            x = getattr(self, f'encoder_block_{i}')(x)\n",
    "            self.s.append(x) # store intermediate output as skip connection\n",
    "        return x\n",
    "    \n",
    "    def forward_skip(self):\n",
    "        \"x1 -> x1', x2 -> x2', .. , xn -> xn'\"\n",
    "        for i in range(self.n_blocks - 1):\n",
    "            # self.s[0] is the input, not the output from the first encoder block\n",
    "            # so self.s[1] will yield the output from encoder_block_0 for skip_block_0\n",
    "            self.s[i+1] = getattr(self, f'skip_block_{i}')(self.s[i+1])\n",
    "    \n",
    "    def forward_middle(self, x):\n",
    "        \" x -> x \"\n",
    "        return self.middle_block(x)\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "        \"x + x1 -> x_up1, x_up1 + x2 -> x_up2, .. , x_up(n-1) + x_n -> x_upn\"\n",
    "        # upsampling\n",
    "        for i in reversed(range(self.n_blocks)):\n",
    "            x = getattr(self, f'decoder_block_{i}')(x, self.s[i]) \n",
    "            # len(s) == (self.n_layers + 1), so s[i] will yield the right skip connection\n",
    "            self.s[i] = (x) # store output from U-Net Block for use in e.g. Deep Supervision\n",
    "\n",
    "        # self.s = [x_up1, x_up2, .., x_upn, xn], where xn is the output from `self.middle_block`\n",
    "        # xn is not needed anymore and can be removed\n",
    "        self.s = self.s[:-1]\n",
    "        return x\n",
    "    \n",
    "    def forward_extra_after_decoder(self):\n",
    "        \"x_up1 -> x_up1', x_up2 -> x_up2', .. , x_upn -> x_upn'\"\n",
    "        for i in reversed(range(self.n_blocks)):\n",
    "            # self.s[0] is the out, not the output from the last decoder block (decoder_block_0)\n",
    "            # so self.s[n_blocks]  is the output of the first decoder block (decoder_block_(n_blocks))\n",
    "            self.s[i] = getattr(self, f'extra_after_decoder_block_{i}')(self.s[i])\n",
    "        \n",
    "    def forward_final(self, x):\n",
    "        \"x -> classes\"\n",
    "        # final layer\n",
    "        return self.final_block(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sz = x.shape[-3:] # store size for resizing\n",
    "        \n",
    "        x = self.forward_encoder(x)\n",
    "        self.forward_skip()\n",
    "        x = self.forward_middle(x)\n",
    "#         x = self.forward_extra_after_skip(x) # maybe added later. A global layer with access to all tensors sored in self.s\n",
    "        x = self.forward_decoder(x)\n",
    "        self.forward_extra_after_decoder()\n",
    "        # if `forward_extra_after_decoder` changed self.s, then the combined tensors of \n",
    "        # self.s are passed to the final block. For this they need to be resized and concatenated\n",
    "        if not isinstance(self.extra_after_decoder_block_0, nn.Identity): \n",
    "            self.s = [F.interpolate(s, sz, mode='nearest') for s in self.s]\n",
    "            x = torch.cat(self.s, 1)\n",
    "        x = self.forward_final(x)\n",
    "        \n",
    "        # final resize\n",
    "        if x.shape[-3:] != sz: \n",
    "            x = F.interpolate(x, sz, mode='nearest')\n",
    "        return x\n",
    "            \n",
    "    def _not_implemented_error(self, layer_name): \n",
    "        msg1, msg2 = 'For example:\\n@patch\\ndef', '(self:ModularUNet, **kwargs): return' \n",
    "        if layer_name == 'encoder_layer': \n",
    "            patch = 'BasicResBlock(**kwargs)'\n",
    "        elif layer_name == 'middle_layer': \n",
    "            patch = 'SqueezeExpand(**kwargs, se_ratio=0.2)'\n",
    "        elif layer_name == 'decoder_layer': \n",
    "            patch = 'UnetBlock(**kwargs)'\n",
    "        else: patch='nn.Identity()'\n",
    "        raise NotImplementedError(f'self.{layer_name} is not implemented for `ModularUNet`. '\n",
    "                                  f'It can be added using the `@patch` decorator or by subclassing `ModularUNet`'\n",
    "                                  f'{msg1} {layer_name} {msg2} {patch}'\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc766cec-4861-4b3e-af0c-08aeb3b38afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted blocks.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted modular_unet.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f70e4d-22b3-44e7-9cbc-33ecefd0888d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
