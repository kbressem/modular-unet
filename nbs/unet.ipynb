{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "\n",
    "Implementation of the fastai [Dynamic U-Net](https://docs.fast.ai/vision.models.unet.html) for threedimensional inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp unet\n",
    "# export\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from attention_unet.utils import create_body, in_channels, model_sizes, _get_sz_change_idxs, dummy_eval\n",
    "from attention_unet.fastai_hooks import hook_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.video import r3d_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_3d = create_body(r3d_18, n_in = 3, pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ConvLayer(nn.Sequential):\n",
    "    \"Create a sequence of convolutional layer, normalization layer and activation function\"\n",
    "    def __init__(self, in_channels, out_channels=None, kernel_size=3, stride=1, padding=None, \n",
    "                 blur=False, act_cls=nn.ReLU, norm_type=None, transpose=False, **kwargs):\n",
    "        super().__init__()\n",
    "        if not out_channels: out_channels = in_channels\n",
    "        if padding is None: padding = ((kernel_size-1)//2 if not transpose else 0)    \n",
    "        conv = nn.ConvTranspose3d if transpose else nn.Conv3d\n",
    "        layers = [conv(in_channels, out_channels, kernel_size, stride, padding, **kwargs)]\n",
    "        if norm_type: layers += [getattr(nn, f'{norm_type}Norm3d')(out_channels)]\n",
    "        if act_cls: layers += [act_cls()]\n",
    "        if blur: \n",
    "            blur = [nn.ReplicationPad3d((1,0,1,0,1,0)), nn.AvgPool3d(2, stride=1)]\n",
    "            if transpose: layers += blur\n",
    "            else: layers = [*blur, *layers]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sequence of convolutional layer, normalization layer and activation function. \n",
    "\n",
    "**Args**\n",
    "\n",
    "| name              | type              | description                                                                                                      |\n",
    "|-------------------|-------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| in_channels       | int               | Number of input channels to convolutional layer                                                                  |\n",
    "| out_channels      | int               | Number of output channels to convolutional layer                                                                 |\n",
    "| kernel_size       | int or int-tuple  | Size of convolutional kernel                                                                                     |\n",
    "| stride            | int or int-tuple  | Stride of the convolutional kernel                                                                               |\n",
    "| padding           | None, int or int-tuple | Padding during convolution. If `None` padding is estimated automatically                                    |\n",
    "| act_cls           | nn.Module         | The activation function to be used. Default `nn.ReLU`                                                            |\n",
    "| norm_type         | str               | The normalization layer to be used                                                                               |\n",
    "| blur              | bool              | Blur the output after upsampling                                                                                 |\n",
    "| transpose         | bool              | Make convolutional layer a transposed convolution                                                                |\n",
    "| kwargs            | -                 | Further arguments passed to the convolutional layer                                                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"Apply attention gate to input in U-Net Block. Adapted from arxiv.org/abs/1804.03999\"\n",
    "    def __init__(self, up_channels, gated_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv_up = nn.Conv3d(up_channels, gated_channels, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
    "        self.conv_s = nn.Conv3d(gated_channels, gated_channels, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias = False)\n",
    "        self.conv_both = nn.Sequential(\n",
    "            nn.ReLU(), \n",
    "            nn.Conv3d(gated_channels, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "           \n",
    "    def forward(self, up_in, s):\n",
    "        x = self.conv_up(up_in)\n",
    "        s = F.interpolate(self.conv_s(s), size=x.shape[2:], mode='trilinear', align_corners=False)\n",
    "        attn_gate = F.interpolate(self.conv_both(x + s), size=up_in.shape[2:], mode='trilinear', align_corners=False)\n",
    "        return up_in * attn_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply attention gate to input in U-Net Block. Adapted from arxiv.org/abs/1804.03999\n",
    "\n",
    "| name              | type              | description                                                                                                      |\n",
    "|-------------------|-------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| up_channels       | int               | Number of channels in tensor to be upsampled. Attention gate will be applied to this tensor                      |\n",
    "| gated_channel     | int               | Number of channels in gated input (from skip connection)                                                         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UnetBlock3D(nn.Module):\n",
    "    \"Create a U-Net Block, optional with spatial attention\"\n",
    "    def __init__(self, up_channels, gated_channels, hook, final_div=True, blur=False, act_cls=nn.ReLU,\n",
    "                 norm_type=None, attention_gate=False, **kwargs):\n",
    "        super(UnetBlock3D, self).__init__()\n",
    "        self.hook = hook\n",
    "        self.up = ConvLayer(up_channels, up_channels//2, kernel_size=3, stride=2, blur=blur, act_cls=act_cls, norm_type=norm_type, transpose=True, **kwargs)\n",
    "        self.bn = getattr(nn, f'{norm_type}Norm3d')(gated_channels)\n",
    "        self.attention_gate = attention_gate\n",
    "        if attention_gate: \n",
    "            self.spatial_attention = SpatialAttention(up_channels, gated_channels)\n",
    "            \n",
    "        in_channels = up_channels//2 + gated_channels\n",
    "        out_channels = in_channels if final_div else in_channels//2\n",
    "        \n",
    "        self.final_conv = nn.Sequential(\n",
    "            act_cls(),\n",
    "            ConvLayer(in_channels, out_channels, act_cls=act_cls, norm_type=norm_type, **kwargs),\n",
    "            ConvLayer(out_channels, out_channels, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        )\n",
    "\n",
    "    def forward(self, up_in):\n",
    "        s = self.bn(self.hook.stored)\n",
    "        if self.attention_gate: up_in = self.spatial_attention(up_in, s)\n",
    "        up_out = self.up(up_in)\n",
    "        if s.shape[-3:] != up_out.shape[-3:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-3:], mode='nearest')\n",
    "        cat_x = torch.cat([up_out, s], dim=1)\n",
    "        return self.final_conv(cat_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a U-Net Block, optional with spatial attention\n",
    "\n",
    "**Args**\n",
    "\n",
    "| name              | type              | description                                                                                                      |\n",
    "|-------------------|-------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| up_channels       | int               | Number of channels in tensor to be upsampled                                                                     |\n",
    "| gated_channels    | int               | Number of channels in gated input (from skip connection)                                                         |\n",
    "| hook              | hook              | Hooked output from encoder layer (implementation of skip connection)                                             |\n",
    "| final_div         | bool              | ?????                                                                                                            |\n",
    "| blur              | bool              | Blur the output after upsampling                                                                                 |\n",
    "| act_cls           | nn.Module         | The activation function to be used. Default `nn.ReLU`                                                            |\n",
    "| norm_type         | str               | The normalization layer to be used                                                                               |\n",
    "| attention_gate    | bool              | Use spatial attention in UNet-Block, adapted from arxiv.org/abs/1804.03999                                       |\n",
    "| kwargs            | -                 | Further arguments passed to the convolutional layers                                                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DeepSupervision(nn.Module):\n",
    "    \"Create segmentation mask from input as described in arxiv.org/abs/1701.03056  \"\n",
    "    def __init__(self, in_channels, out_channels, img_size, **kwargs):\n",
    "        super(DeepSupervision, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, **kwargs)\n",
    "        self.img_size=img_size\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.conv(x)\n",
    "        return F.interpolate(x, self.img_size, mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create segmentation mask from input as described in arxiv.org/abs/1701.03056\n",
    "\n",
    "**Args**\n",
    "\n",
    "| name              | type              | description                                                                                                      |\n",
    "|-------------------|-------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| in_channels       | int               | Number of input channels to convolutional layer                                                                  |\n",
    "| out_channels      | int               | Number of classes                                                                                                |\n",
    "| img_size          | tuple             | Resolution of the input as tuple with len 3                                                                      |\n",
    "| kwargs            | -                 | Further arguments passed to the convolutional layers                                                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SqueezeExcitation(nn.Module): \n",
    "    \"Squeeze-and-Excitation layer as described in arxiv.org/pdf/1709.01507.pdf \"\n",
    "    # ToDo: evaluate final BN - differences for training?\n",
    "    def __init__(self, in_channels, se_ratio=0.15, act_cls=nn.ReLU, norm_type='None', **kwargs):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        \n",
    "        assert 0 < se_ratio <= 1, f'Expected `se_ratio` to be between 0 and 1 but got {se_ratio}'\n",
    "        num_squeezed_channels = max(1, int(in_channels * se_ratio))\n",
    "\n",
    "        \n",
    "        self.squeeze_expand = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            ConvLayer(in_channels=in_channels, out_channels=num_squeezed_channels, kernel_size=1,\n",
    "                  act_cls=act_cls, norm_type=None, **kwargs), \n",
    "            ConvLayer(in_channels=num_squeezed_channels, out_channels=in_channels, kernel_size=1,\n",
    "                  act_cls=nn.Sigmoid, norm_type=None,**kwargs), \n",
    "            )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.squeeze_expand(x) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squeeze-and-Excitation layer as described in arxiv.org/pdf/1709.01507.pdf \n",
    "**Args**\n",
    "\n",
    "| name              | type              | description                                                                                                      |\n",
    "|-------------------|-------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| in_channels       | int               | Number of input channels to first convolutional layer                                                            |\n",
    "| se_ratio          | float             | Squeeze-Expand ratio, should be a float value between 0 and 1                                                    | \n",
    "| act_cls           | nn.Module         | The activation function to be used. Default `nn.ReLU`                                                            |\n",
    "| norm_type         | str               | The normalization layer to be used                                                                               |\n",
    "| kwargs            | -                 | Further arguments passed to the convolutional layers                                                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DynamicUnet3D(nn.Module):\n",
    "    \n",
    "    # To Do Init layers properly\n",
    "    \" Create a U-Net from a given architecture \"\n",
    "    def __init__(self, encoder, n_classes, img_size, act_cls=nn.ReLU, norm_type='Batch', \n",
    "                 blur=False, deep_supervision=False, se_middle_conv=False, se_ratio=0.15, attention_gate=False, **kwargs):\n",
    "        super(DynamicUnet3D, self).__init__()\n",
    "        \n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        # examine encoder and place hooks after each major block\n",
    "        sizes = model_sizes(encoder, size=img_size) \n",
    "        sz_chg_idxs = list(reversed(_get_sz_change_idxs(sizes)))        \n",
    "        if not 0 in sz_chg_idxs and attention_gate: sz_chg_idxs += [0] # adds an extra U-Net Block with higher resolution attn_gate\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sz_chg_idxs], detach=False)\n",
    "        x = dummy_eval(encoder, img_size).detach() \n",
    "        \n",
    "        # construct middle layer\n",
    "        input_channels = x.size(1)\n",
    "        if se_middle_conv: \n",
    "            middle_conv = SqueezeExcitation(input_channels, se_ratio=se_ratio, act_cls=act_cls, norm_type=norm_type)\n",
    "        else: \n",
    "            middle_conv = nn.Sequential(\n",
    "                ConvLayer(input_channels, input_channels*2, act_cls=act_cls, norm_type=norm_type, **kwargs),\n",
    "                ConvLayer(input_channels*2, input_channels, act_cls=act_cls, norm_type=norm_type, **kwargs)).eval()  \n",
    "        \n",
    "        x = middle_conv(x)\n",
    "        self.encoder = encoder\n",
    "        self.middle_conv = middle_conv\n",
    "\n",
    "        # create upsample blocks/U-Net Block\n",
    "        layers, ds = [], []\n",
    "        for i,idx in enumerate(sz_chg_idxs):\n",
    "            not_final = i!=len(sz_chg_idxs)-1\n",
    "            up_channels, gated_channels = int(x.shape[1]), int(sizes[idx][1])\n",
    "            unet_block = UnetBlock3D(up_channels, gated_channels, self.sfs[i], final_div=not_final, attention_gate=attention_gate, blur=blur if not_final else not blur, \n",
    "                                     act_cls=act_cls, norm_type=norm_type, **kwargs).eval()\n",
    "            layers.append(unet_block)\n",
    "            x = unet_block(x)\n",
    "            if self.deep_supervision: ds.append(DeepSupervision(x.shape[1], n_classes, img_size))\n",
    "                    \n",
    "        # add another TransposedConv Layer if the Input is still to small\n",
    "        input_channels = x.size(1)\n",
    "        if img_size != sizes[0][-3:]: \n",
    "            layers.append(ConvLayer(input_channels, input_channels//2, kernel_size=(1,3,3), act_cls=act_cls, norm_type=norm_type, stride=(1,2,2), transpose=True))\n",
    "            input_channels = input_channels//2\n",
    "            if self.deep_supervision: ds.append(DeepSupervision(input_channels, n_classes, img_size))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        if self.deep_supervision: self.ds = nn.ModuleList(ds)\n",
    "\n",
    "        # Construct the final layer\n",
    "        self.final = ConvLayer(n_classes * len(ds) if self.deep_supervision else input_channels, n_classes, kernel_size=1, stride=1, act_cls=None, norm_type=None,  **kwargs)        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        sz = x.shape[2:]\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle_conv(x)\n",
    "        ds_masks=[]\n",
    "        for i,l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "            if self.deep_supervision: \n",
    "                ds_masks.append(self.ds[i](x))\n",
    "                \n",
    "        x = self.final(torch.cat(ds_masks, 1) if self.deep_supervision else x) \n",
    "        x = F.interpolate(x, sz, mode='nearest')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a U-Net from a given architecture, based on fastai DynamicUnet\n",
    "\n",
    "**Args**\n",
    "\n",
    "| name              | type              | description                                                                                                      |\n",
    "|-------------------|-------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| encoder           | nn.Sequential     | The encoder architecture as created by `utils.create_body`                                                       |\n",
    "| n_classes         | int               | Number of classes                                                                                                |\n",
    "| img_size          | tuple             | Resolution of the input as tuple with len 3                                                                      |\n",
    "| act_cls           | nn.Module         | The activation function to be used. Default `nn.ReLU`                                                            |\n",
    "| norm_type         | str               | The normalization layer to be used                                                                               |\n",
    "| blur              | bool              | Blur the output after upsampling                                                                                 |\n",
    "| deep_supervision  | bool              | Use deep supervision as described in arxiv.org/abs/1701.03056                                                    |   \n",
    "| se_middle_conv    | bool              | Add channel wise attention to the middle convolution with a Squeeze-and-Excitation layer arxiv.org/pdf/1709.01507.pdf |\n",
    "| se_ratio          | float             | Squeeze-Expand ratio, should be a float value between 0 and 1                                                    | \n",
    "| attention_gate    | bool              | Use spatial attention in UNet-Block, adapted from arxiv.org/abs/1804.03999                                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def uresnet_18(in_channels, n_classes, img_size, pretrained=True, **kwargs): \n",
    "    arch = create_body(r3d_18, n_in=in_channels, pretrained=pretrained)\n",
    "    unet = DynamicUnet3D(arch, n_classes=n_classes, img_size=img_size)\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted fastai-hooks.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted unet.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
