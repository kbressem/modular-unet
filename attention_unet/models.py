# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/models.ipynb (unless otherwise specified).

__all__ = ['UResNet', 'UResNet18', 'UResNet34', 'UResNet18WithAttention']

# Cell
# default_exp models
import torch
from torch import nn
from fastcore.dispatch import patch
from fastcore.basics import store_attr

# Cell
import sys
sys.path.append('..')
from .modular_unet import ModularUNet
from .blocks import BasicResBlock, SqueezeExpand, UnetBlock
from .utils import test_forward

# Cell
class UResNet(ModularUNet):
    def encoder_layer(self, **kwargs): return BasicResBlock(**kwargs)
    def middle_layer(self, **kwargs): return SqueezeExpand(**kwargs, se_ratio=0.2)
    def skip_layer(self, **kwargs): return nn.Identity()
    def decoder_layer(self, **kwargs): return UnetBlock(**kwargs)
    def extra_after_decoder_layer(self, **kwargs): return nn.Identity()
    def final_layer(self, **kwargs): return BasicResBlock(**kwargs)

# Cell
class UResNet18(UResNet):
    " UNet with ResNet18-like Backbone "
    channels = 32, 64, 128, 256, 512
    kernel_size = 3, 3, 3, 3, 3
    stride = 2, 2, 2, 2, 2
    padding = 'auto', 'auto', 'auto', 'auto', 'auto'
    n_layers = 1, 2, 2, 2, 2
    n_blocks = 5

# Cell
class UResNet34(UResNet18):
    " UNet with ResNet34-like Backbone "
    channels = 32, 64, 128, 256, 512
    kernel_size = 3, 3, 3, 3, 3
    stride = 2, 2, 2, 2, 2
    padding = 'auto', 'auto', 'auto', 'auto', 'auto'
    n_layers = 1, 3, 4, 6, 3
    n_blocks = 5

# Cell
class UResNet18WithAttention(UResNet18):
    " UNet with ResNet18-like Backbone and spatial Attention in Upsampling blocks"
    pass

# Cell
@patch
def decoder_layer(self:UResNet18WithAttention, **kwargs):
    return UnetBlock(spatial_attention=True, **kwargs)