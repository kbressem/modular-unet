# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/unet.ipynb (unless otherwise specified).

__all__ = ['ConvLayer', 'SpatialAttention', 'UnetBlock3D', 'DeepSupervision', 'SqueezeExcitation', 'DynamicUnet3D']

# Cell
# export
import torch
from torch import nn
import torch.nn.functional as F
from .utils import create_body, in_channels, model_sizes, _get_sz_change_idxs, dummy_eval
from .fastai_hooks import hook_outputs

# Cell
class ConvLayer(nn.Sequential):
    "Create a sequence of convolutional layer, normalization layer and activation function"
    def __init__(self, in_channels, out_channels=None, kernel_size=3, stride=1, padding=None,
                 blur=False, act_cls=nn.ReLU, norm_type=None, transpose=False, **kwargs):
        super().__init__()
        if not out_channels: out_channels = in_channels
        if padding is None: padding = ((kernel_size-1)//2 if not transpose else 0)
        conv = nn.ConvTranspose3d if transpose else nn.Conv3d
        layers = [conv(in_channels, out_channels, kernel_size, stride, padding, **kwargs)]
        if norm_type: layers += [getattr(nn, f'{norm_type}Norm3d')(out_channels)]
        if act_cls: layers += [act_cls()]
        if blur:
            blur = [nn.ReplicationPad3d((1,0,1,0,1,0)), nn.AvgPool3d(2, stride=1)]
            if transpose: layers += blur
            else: layers = [*blur, *layers]
        super().__init__(*layers)

# Cell
class SpatialAttention(nn.Module):
    "Apply attention gate to input in U-Net Block. Adapted from arxiv.org/abs/1804.03999"
    def __init__(self, up_channels, gated_channels):
        super(SpatialAttention, self).__init__()
        self.conv_up = nn.Conv3d(up_channels, gated_channels, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        self.conv_s = nn.Conv3d(gated_channels, gated_channels, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias = False)
        self.conv_both = nn.Sequential(
            nn.ReLU(),
            nn.Conv3d(gated_channels, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1)),
            nn.Sigmoid()
        )

    def forward(self, up_in, s):
        x = self.conv_up(up_in)
        s = F.interpolate(self.conv_s(s), size=x.shape[2:], mode='trilinear', align_corners=False)
        attn_gate = F.interpolate(self.conv_both(x + s), size=up_in.shape[2:], mode='trilinear', align_corners=False)
        return up_in * attn_gate

# Cell
class UnetBlock3D(nn.Module):
    "Create a U-Net Block, optional with spatial attention"
    def __init__(self, up_channels, gated_channels, hook, final_div=True, blur=False, act_cls=nn.ReLU,
                 norm_type=None, attention_gate=False, **kwargs):
        super(UnetBlock3D, self).__init__()
        self.hook = hook
        self.up = ConvLayer(up_channels, up_channels//2, kernel_size=3, stride=2, blur=blur, act_cls=act_cls, norm_type=norm_type, transpose=True, **kwargs)
        self.bn = getattr(nn, f'{norm_type}Norm3d')(gated_channels)
        self.attention_gate = attention_gate
        if attention_gate:
            self.spatial_attention = SpatialAttention(up_channels, gated_channels)

        in_channels = up_channels//2 + gated_channels
        out_channels = in_channels if final_div else in_channels//2

        self.final_conv = nn.Sequential(
            act_cls(),
            ConvLayer(in_channels, out_channels, act_cls=act_cls, norm_type=norm_type, **kwargs),
            ConvLayer(out_channels, out_channels, act_cls=act_cls, norm_type=norm_type, **kwargs)
        )

    def forward(self, up_in):
        s = self.bn(self.hook.stored)
        if self.attention_gate: up_in = self.spatial_attention(up_in, s)
        up_out = self.up(up_in)
        if s.shape[-3:] != up_out.shape[-3:]:
            up_out = F.interpolate(up_out, s.shape[-3:], mode='nearest')
        cat_x = torch.cat([up_out, s], dim=1)
        return self.final_conv(cat_x)

# Cell
class DeepSupervision(nn.Module):
    "Create segmentation mask from input as described in arxiv.org/abs/1701.03056  "
    def __init__(self, in_channels, out_channels, img_size, **kwargs):
        super(DeepSupervision, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, **kwargs)
        self.img_size=img_size

    def forward(self, x):
        x = self.conv(x)
        return F.interpolate(x, self.img_size, mode='nearest')

# Cell
class SqueezeExcitation(nn.Module):
    "Squeeze-and-Excitation layer as described in arxiv.org/pdf/1709.01507.pdf "
    # ToDo: evaluate final BN - differences for training?
    def __init__(self, in_channels, se_ratio=0.15, act_cls=nn.ReLU, norm_type='None', **kwargs):
        super(SqueezeExcitation, self).__init__()

        assert 0 < se_ratio <= 1, f'Expected `se_ratio` to be between 0 and 1 but got {se_ratio}'
        num_squeezed_channels = max(1, int(in_channels * se_ratio))


        self.squeeze_expand = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            ConvLayer(in_channels=in_channels, out_channels=num_squeezed_channels, kernel_size=1,
                  act_cls=act_cls, norm_type=None, **kwargs),
            ConvLayer(in_channels=num_squeezed_channels, out_channels=in_channels, kernel_size=1,
                  act_cls=nn.Sigmoid, norm_type=None,**kwargs),
            )

    def forward(self, x):
        return self.squeeze_expand(x) * x

# Cell
class DynamicUnet3D(nn.Module):

    # To Do Init layers properly
    " Create a U-Net from a given architecture, based on fastai DynamicUnet "
    def __init__(self, encoder, n_classes, img_size, act_cls=nn.ReLU, norm_type='Batch',
                 blur=False, deep_supervision=False, se_middle_conv=False, se_ratio=0.15, attention_gate=False, **kwargs):
        super(DynamicUnet3D, self).__init__()

        self.deep_supervision = deep_supervision

        # examine encoder and place hooks after each major block
        sizes = model_sizes(encoder, size=img_size)
        sz_chg_idxs = list(reversed(_get_sz_change_idxs(sizes)))
        if not 0 in sz_chg_idxs and attention_gate: sz_chg_idxs += [0] # adds an extra U-Net Block with higher resolution attn_gate
        self.sfs = hook_outputs([encoder[i] for i in sz_chg_idxs], detach=False)
        x = dummy_eval(encoder, img_size).detach()

        # construct middle layer
        input_channels = x.size(1)
        if se_middle_conv:
            middle_conv = SqueezeExcitation(input_channels, se_ratio=se_ratio, act_cls=act_cls, norm_type=norm_type)
        else:
            middle_conv = nn.Sequential(
                ConvLayer(input_channels, input_channels*2, act_cls=act_cls, norm_type=norm_type, **kwargs),
                ConvLayer(input_channels*2, input_channels, act_cls=act_cls, norm_type=norm_type, **kwargs)).eval()

        x = middle_conv(x)
        self.encoder = encoder
        self.middle_conv = middle_conv

        # create upsample blocks/U-Net Block
        layers, ds = [], []
        for i,idx in enumerate(sz_chg_idxs):
            not_final = i!=len(sz_chg_idxs)-1
            up_channels, gated_channels = int(x.shape[1]), int(sizes[idx][1])
            unet_block = UnetBlock3D(up_channels, gated_channels, self.sfs[i], final_div=not_final, attention_gate=attention_gate, blur=blur if not_final else not blur,
                                     act_cls=act_cls, norm_type=norm_type, **kwargs).eval()
            layers.append(unet_block)
            x = unet_block(x)
            if self.deep_supervision: ds.append(DeepSupervision(x.shape[1], n_classes, img_size))

        # add another TransposedConv Layer if the Input is still to small
        input_channels = x.size(1)
        if img_size != sizes[0][-3:]:
            layers.append(ConvLayer(input_channels, input_channels//2, kernel_size=(1,3,3), act_cls=act_cls, norm_type=norm_type, stride=(1,2,2), transpose=True))
            input_channels = input_channels//2
            if self.deep_supervision: ds.append(DeepSupervision(input_channels, n_classes, img_size))

        self.layers = nn.ModuleList(layers)
        if self.deep_supervision: self.ds = nn.ModuleList(ds)

        # Construct the final layer
        self.final = ConvLayer(n_classes * len(ds) if self.deep_supervision else input_channels, n_classes, kernel_size=1, stride=1, act_cls=None, norm_type=None,  **kwargs)

    def forward(self, x):
        sz = x.shape[2:]
        x = self.encoder(x)
        x = self.middle_conv(x)
        ds_masks=[]
        for i,l in enumerate(self.layers):
            x = l(x)
            if self.deep_supervision:
                ds_masks.append(self.ds[i](x))

        x = self.final(torch.cat(ds_masks, 1) if self.deep_supervision else x)
        x = F.interpolate(x, sz, mode='nearest')

        return x